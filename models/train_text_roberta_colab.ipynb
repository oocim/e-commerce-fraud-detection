{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04882fb6",
   "metadata": {},
   "source": [
    "# Fraud Detection — Hybrid TF-IDF + RoBERTa (Colab)\n",
    "\n",
    "This notebook uploads `text_dataset.csv`, performs a stratified train/test split, extracts **TF-IDF features** and **RoBERTa contextual embeddings**, fuses them in a hybrid model, and reports evaluation metrics.\n",
    "\n",
    "**Architecture:** Text → TF-IDF (5000-d → 256-d projection) + RoBERTa [CLS] (768-d) → Concatenate (1024-d) → Classifier → Fraud / Not Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab-compatible — use Colab's pre-installed pandas/numpy)\n",
    "!pip -q install transformers scikit-learn scipy joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import scipy.sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716053b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "OUTPUT_DIR = Path('./roberta_fraud_model')\n",
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM = 4\n",
    "EPOCHS = 5           # restored — 3 was too few, model barely learned\n",
    "LR = 2e-5            # restored — 1e-5 was too slow for 5 epochs\n",
    "DROPOUT = 0.3        # restored — 0.4 was too aggressive with layer freezing\n",
    "LABEL_SMOOTHING = 0.05  # reduced from 0.1 — less target softening\n",
    "SEED = 42\n",
    "SYNTH_CAP = 150      # max synthetic fraud rows to include (was 500)\n",
    "\n",
    "# TF-IDF config\n",
    "TFIDF_MAX_FEATURES = 2000    # reduced from 5000 — fewer features = less overfitting\n",
    "TFIDF_NGRAM_RANGE = (1, 2)   # unigrams + bigrams\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "print('\\nNext cell will ask you to upload text_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and load text_dataset.csv + synthetic_text_dataset.csv\n",
    "if IN_COLAB:\n",
    "    print('Upload text_dataset.csv and synthetic_text_dataset.csv ...')\n",
    "    uploaded = files.upload()\n",
    "    if 'text_dataset.csv' not in uploaded:\n",
    "        raise ValueError(f'Expected text_dataset.csv. Uploaded: {list(uploaded.keys())}')\n",
    "    df = pd.read_csv(io.BytesIO(uploaded['text_dataset.csv']))\n",
    "    if 'synthetic_text_dataset.csv' in uploaded:\n",
    "        synth_df = pd.read_csv(io.BytesIO(uploaded['synthetic_text_dataset.csv']))\n",
    "    else:\n",
    "        raise ValueError('Also upload synthetic_text_dataset.csv')\n",
    "else:\n",
    "    df = pd.read_csv('processed_data/text_dataset.csv')\n",
    "    synth_df = pd.read_csv('processed_data/synthetic_text_dataset.csv')\n",
    "\n",
    "print(f'Original dataset: {df.shape}  |  Synthetic (fraud): {synth_df.shape}')\n",
    "print(f'Original fraud dist:\\n{df[\"fraud_label\"].value_counts()}')\n",
    "\n",
    "# Cap synthetic data to prevent overfitting\n",
    "# The full 500 synthetic rows (all fraud) push fraud to 20.7%, which was\n",
    "# too aggressive. Capping at SYNTH_CAP gives a more balanced boost.\n",
    "if len(synth_df) > SYNTH_CAP:\n",
    "    synth_df = synth_df.sample(n=SYNTH_CAP, random_state=SEED)\n",
    "    print(f'\\nCapped synthetic data to {SYNTH_CAP} rows (from {len(synth_df)})')\n",
    "\n",
    "# Combine original + capped synthetic\n",
    "df = pd.concat([df, synth_df], ignore_index=True)\n",
    "n_total = len(df)\n",
    "n_fraud = df['fraud_label'].sum()\n",
    "print(f'\\nCombined dataset: {df.shape}')\n",
    "print(f'Combined fraud dist:\\n{df[\"fraud_label\"].value_counts()}')\n",
    "print(f'Fraud ratio: {100*n_fraud/n_total:.1f}%\\n')\n",
    "\n",
    "text_cols = ['title_cleaned', 'description_cleaned', 'review1_cleaned', 'review2_cleaned']\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "df['text'] = (\n",
    "    'Title: ' + df['title_cleaned']\n",
    "    + ' Description: ' + df['description_cleaned']\n",
    "    + ' Review1: ' + df['review1_cleaned']\n",
    "    + ' Review2: ' + df['review2_cleaned']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split (stratified)\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df['fraud_label']\n",
    ")\n",
    "print(f'Train: {len(train_df)} | Test: {len(test_df)}')\n",
    "print(f'Train fraud ratio: {train_df[\"fraud_label\"].mean():.4f}')\n",
    "print(f'Test  fraud ratio: {test_df[\"fraud_label\"].mean():.4f}\\n')\n",
    "\n",
    "# Fit TF-IDF on training text only (prevent data leakage)\n",
    "print('Fitting TF-IDF vectorizer on training set ...')\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=TFIDF_MAX_FEATURES,\n",
    "    ngram_range=TFIDF_NGRAM_RANGE,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    ")\n",
    "tfidf_train = tfidf.fit_transform(train_df['text']).toarray().astype(np.float32)\n",
    "tfidf_test  = tfidf.transform(test_df['text']).toarray().astype(np.float32)\n",
    "\n",
    "TFIDF_DIM = tfidf_train.shape[1]\n",
    "print(f'TF-IDF vocabulary size: {len(tfidf.vocabulary_)}')\n",
    "print(f'TF-IDF feature dimension: {TFIDF_DIM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d92266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer & dataset (with TF-IDF features)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class FraudTextDataset(Dataset):\n",
    "    \"\"\"Dataset that returns RoBERTa tokens + TF-IDF features.\"\"\"\n",
    "    def __init__(self, texts, labels, tfidf_matrix):\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.tfidf  = torch.tensor(tfidf_matrix, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids':      self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'tfidf_features': self.tfidf[idx],\n",
    "            'labels':         self.labels[idx],\n",
    "        }\n",
    "\n",
    "print('Tokenizing training set ...')\n",
    "train_dataset = FraudTextDataset(\n",
    "    train_df['text'].tolist(), train_df['fraud_label'].tolist(), tfidf_train\n",
    ")\n",
    "print('Tokenizing test set ...')\n",
    "test_dataset = FraudTextDataset(\n",
    "    test_df['text'].tolist(), test_df['fraud_label'].tolist(), tfidf_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid model: RoBERTa [CLS] + TF-IDF → classifier\n",
    "\n",
    "class RoBERTaTfidfFraudModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Concatenates RoBERTa's [CLS] embedding (768-d) with TF-IDF features\n",
    "    before a shared classification head.\n",
    "\n",
    "    Anti-overfitting measures:\n",
    "      - Freeze all RoBERTa layers except the last 4 encoder blocks + pooler\n",
    "      - Dropout (0.3) in projection and classifier\n",
    "      - Light label smoothing (0.05) in the loss function\n",
    "    Note: The main overconfidence fix is NO class_weight + capped synthetic data.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, tfidf_dim, num_labels=2, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        roberta_dim = self.roberta.config.hidden_size  # 768\n",
    "\n",
    "        # Freeze all RoBERTa except last 4 encoder layers + pooler\n",
    "        # Last-2 was too aggressive — model couldn't learn enough signal.\n",
    "        for name, param in self.roberta.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze last 4 encoder layers (layers 8-11 of 0-11)\n",
    "        for layer in self.roberta.encoder.layer[-4:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        # Unfreeze pooler if it exists\n",
    "        if hasattr(self.roberta, 'pooler') and self.roberta.pooler is not None:\n",
    "            for param in self.roberta.pooler.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.tfidf_proj = nn.Sequential(\n",
    "            nn.Linear(tfidf_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        fused_dim = roberta_dim + 256  # 768 + 256 = 1024\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tfidf_features, labels=None):\n",
    "        # RoBERTa [CLS] embedding\n",
    "        roberta_out = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = roberta_out.last_hidden_state[:, 0, :]  # [batch, 768]\n",
    "\n",
    "        # Project TF-IDF to 256-d\n",
    "        tfidf_emb = self.tfidf_proj(tfidf_features)        # [batch, 256]\n",
    "\n",
    "        # Concatenate and classify\n",
    "        fused = torch.cat([cls_emb, tfidf_emb], dim=-1)    # [batch, 1024]\n",
    "        logits = self.classifier(fused)                     # [batch, 2]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Light label smoothing prevents extreme overconfidence.\n",
    "            # With 0.05: [0,1] → [0.025, 0.975] — barely noticeable but effective.\n",
    "            loss_fn = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "\n",
    "# Print class distribution\n",
    "n_neg = (train_df['fraud_label'] == 0).sum()\n",
    "n_pos = (train_df['fraud_label'] == 1).sum()\n",
    "print(f'Class distribution: not-fraud={n_neg}, fraud={n_pos} ({100*n_pos/(n_neg+n_pos):.1f}%)')\n",
    "\n",
    "# Instantiate hybrid model\n",
    "model = RoBERTaTfidfFraudModel(MODEL_NAME, tfidf_dim=TFIDF_DIM)\n",
    "model = model.to(device)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "frozen    = total - trainable\n",
    "print(f'Total params:     {total:,}')\n",
    "print(f'Trainable params: {trainable:,} ({100*trainable/total:.1f}%)')\n",
    "print(f'Frozen params:    {frozen:,} ({100*frozen/total:.1f}%)')\n",
    "print(f'Architecture: RoBERTa [CLS](768) + TF-IDF proj(256) → fused(1024) → classifier → 2')\n",
    "print(f'Regularization: dropout={DROPOUT}, label_smoothing={LABEL_SMOOTHING}, freeze=all except last 4 layers')\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    seed=SEED,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds),\n",
    "        'roc_auc': roc_auc_score(labels, probs),\n",
    "        'avg_precision': average_precision_score(labels, probs),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print('=' * 60)\n",
    "print('  Starting RoBERTa fine-tuning')\n",
    "print('=' * 60)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "logits = preds_output.predictions\n",
    "probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n",
    "y_pred = np.argmax(logits, axis=-1)\n",
    "y_true = test_df['fraud_label'].values\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1  = f1_score(y_true, y_pred)\n",
    "roc = roc_auc_score(y_true, probs)\n",
    "ap  = average_precision_score(y_true, probs)\n",
    "\n",
    "print('=' * 60)\n",
    "print('  Final Evaluation — RoBERTa + TF-IDF Hybrid')\n",
    "print('=' * 60)\n",
    "print(f'Accuracy          : {acc:.4f}')\n",
    "print(f'F1 Score (fraud)  : {f1:.4f}')\n",
    "print(f'ROC-AUC           : {roc:.4f}')\n",
    "print(f'Avg Precision (PR): {ap:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, target_names=['Not Fraud', 'Fraud']))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export test-set predictions for ensemble\n",
    "text_preds_df = pd.DataFrame({\n",
    "    'product_id': test_df['product_id'].values,\n",
    "    'fraud_label': y_true,\n",
    "    'text_fraud_proba': probs,\n",
    "    'text_pred': y_pred,\n",
    "})\n",
    "text_preds_df.to_csv('text_test_predictions.csv', index=False)\n",
    "print(f'Saved text_test_predictions.csv  ({len(text_preds_df)} rows)')\n",
    "print(text_preds_df.head())\n",
    "\n",
    "# Download for local ensemble\n",
    "if IN_COLAB:\n",
    "    files.download('text_test_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55844de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model, tokenizer, and TF-IDF vectorizer\n",
    "save_path = OUTPUT_DIR / 'best_model'\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save full hybrid model state dict\n",
    "torch.save(model.state_dict(), save_path / 'hybrid_roberta_tfidf.pth')\n",
    "\n",
    "# Save tokenizer (needed for RoBERTa inference)\n",
    "tokenizer.save_pretrained(str(save_path))\n",
    "\n",
    "# Save TF-IDF vectorizer (needed to transform new text at inference)\n",
    "joblib.dump(tfidf, save_path / 'tfidf_vectorizer.joblib')\n",
    "\n",
    "print(f'Saved to: {save_path.resolve()}')\n",
    "print(f'  - hybrid_roberta_tfidf.pth  (model weights)')\n",
    "print(f'  - tokenizer files           (RoBERTa tokenizer)')\n",
    "print(f'  - tfidf_vectorizer.joblib   (TF-IDF vectorizer)')\n",
    "\n",
    "# Optional: download in Colab\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    shutil.make_archive('roberta_tfidf_model', 'zip', save_path)\n",
    "    files.download('roberta_tfidf_model.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
